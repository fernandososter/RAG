{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "CXBaCN7ZaA2p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXBaCN7ZaA2p",
        "outputId": "c8c6a58b-d9c2-4b91-eb74-5ceb9667f776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t_l08xtsaKP9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_l08xtsaKP9",
        "outputId": "41dfe47d-36dd-4e8c-801b-a1ffc4c0503d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RAG'...\n",
            "remote: Enumerating objects: 20537, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/fernandososter/RAG.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d084432",
      "metadata": {
        "id": "7d084432"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/RAG/QLoRA-Youtube/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7711fb21",
      "metadata": {
        "id": "7711fb21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
        "from peft import PeftModel\n",
        "#from item import Item\n",
        "import pickle\n",
        "from peft import LoraConfig\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cvix6sxJaq1W",
      "metadata": {
        "id": "Cvix6sxJaq1W"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('Tesla T4') >= 0:\n",
        "    print(\"Success - Connected to a T4\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO A T4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d57414d",
      "metadata": {
        "id": "9d57414d"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3241a2f1",
      "metadata": {
        "id": "3241a2f1"
      },
      "outputs": [],
      "source": [
        "PROJECT_PATH = '/content/RAG/QLoRA-Youtube/'\n",
        "MODEL_CACHE_PATH = '/content/drive/MyDrive/modelos/'\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"QLoRA-Youtube\"\n",
        "HF_USER = \"fsoster\"\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}\n",
        "RUN_NAME = \"2025-11-11_13.04.39\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "\n",
        "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
        "\n",
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebba936",
      "metadata": {
        "id": "cebba936"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "class Item:\n",
        "    prompt: Optional[str] = None\n",
        "    PREFIX = \"Views are \"\n",
        "    QUESTION = \"How many views for this video?\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "\n",
        "    title: str\n",
        "    view_count: float\n",
        "\n",
        "    def __init__(self, title, view_count):\n",
        "        self.title = title\n",
        "        self.view_count = view_count\n",
        "        self.makePrompt(self.title)\n",
        "\n",
        "\n",
        "    def makePrompt(self, text):\n",
        "        self.prompt = f\"{self.QUESTION}\\n\\n{text}\\n\\n\"\n",
        "        self.prompt += f\"{self.PREFIX}{str(round(self.view_count))}\"\n",
        "        self.token_count = len(self.tokenizer.encode(self.prompt, add_special_tokens=False))\n",
        "\n",
        "\n",
        "\n",
        "    def test_prompt(self):\n",
        "        return self.prompt.split(self.PREFIX)[0] + self.PREFIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d3408e",
      "metadata": {
        "id": "d0d3408e"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f\"{PROJECT_PATH}/dataset/youtube_video.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc37455",
      "metadata": {
        "id": "7fc37455"
      },
      "outputs": [],
      "source": [
        "columns = [\"title\", \"view_count\"]\n",
        "items = [Item(**item) for item in df[columns].to_dict(orient=\"records\")]\n",
        "test = items[:round(len(items)*0.3)]\n",
        "train = items[round(len(items)*0.3):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0684ed6c",
      "metadata": {
        "id": "0684ed6c"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "pkl_train_file = 'dataset/train.pkl'\n",
        "pkl_test_file = 'dataset/test.pkl'\n",
        "\n",
        "if os.path.exists(pkl_train_file):\n",
        "  with open(pkl_train_file, \"rb\") as f:\n",
        "        train = pickle.load(f)\n",
        "\n",
        "  with open(pkl_test_file, \"rb\") as f:\n",
        "        test = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    columns = [\"title\", \"view_count\"]\n",
        "    items = [Item(**item) for item in df[columns].to_dict(orient=\"records\")]\n",
        "    test = items[:round(len(items)*0.3)]\n",
        "    train = items[round(len(items)*0.3):]\n",
        "    with open(pkl_train_file, 'wb') as file:\n",
        "        pickle.dump(train, file)\n",
        "\n",
        "    with open(pkl_test_file, 'wb') as file:\n",
        "        pickle.dump(test, file)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34637f8",
      "metadata": {
        "id": "f34637f8"
      },
      "outputs": [],
      "source": [
        "len(train), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdaebe0d",
      "metadata": {
        "id": "fdaebe0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01090e34",
      "metadata": {
        "id": "01090e34"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "prompt = train[0].prompt\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=MODEL_CACHE_PATH\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb26c8a",
      "metadata": {
        "id": "1cb26c8a"
      },
      "outputs": [],
      "source": [
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\", # Specifies we're doing causal language modeling\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82806e2f",
      "metadata": {
        "id": "82806e2f"
      },
      "outputs": [],
      "source": [
        "# üì¶ Training Setup:\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 16                     # A100 GPU can go up to 16\n",
        "GRADIENT_ACCUMULATION_STEPS = 2\n",
        "MAX_SEQUENCE_LENGTH = 182          # Max token length per input\n",
        "\n",
        "# ‚öôÔ∏è Optimization:\n",
        "LEARNING_RATE = 1e-4\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "# üíæ Checkpointing & Logging:\n",
        "SAVE_STEPS = 200        # Checkpoint\n",
        "STEPS = 20              # Log every 20 steps\n",
        "save_total_limit = 10   # Keep latest 10 only\n",
        "\n",
        "\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    # Output & Run\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    run_name=RUN_NAME,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "\n",
        "    # Training\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    max_steps=-1,\n",
        "    group_by_length=True,\n",
        "\n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=STEPS,\n",
        "    per_device_eval_batch_size=1,\n",
        "\n",
        "    # Optimization\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    optim=OPTIMIZER,\n",
        "    weight_decay=0.001,\n",
        "    max_grad_norm=0.3,\n",
        "\n",
        "    # Precision\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "\n",
        "    # Logging & Saving\n",
        "    logging_steps=STEPS,            # See loss after each {STEP} batches\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,          # Model Checkpointed locally\n",
        "    save_total_limit=save_total_limit,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "\n",
        "    # Hub\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"end\",  # Only push once, at the end\n",
        "    load_best_model_at_end=True, # Loads the best eval_loss checkpoint\n",
        "    metric_for_best_model=\"eval_loss\", # Monitors eval_loss\n",
        "    greater_is_better=False, # Lower eval_loss = better model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421d7418",
      "metadata": {
        "id": "421d7418"
      },
      "outputs": [],
      "source": [
        "# The latest version of trl is showing a warning about labels - please ignore this warning\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    peft_config=lora_parameters,    # QLoRA config\n",
        "    args=train_parameters,          # SFTConfig\n",
        "    data_collator=collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)] # Early stop if no val improvement for 5 steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480d40ad",
      "metadata": {
        "id": "480d40ad"
      },
      "outputs": [],
      "source": [
        "fine_tuning.train()\n",
        "print(f\"‚úÖ Best model pushed to HF Hub: {HUB_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb474a9",
      "metadata": {
        "id": "4cb474a9"
      },
      "outputs": [],
      "source": [
        "for item in train:\n",
        "    inputs = tokenizer.encode(item.prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
        "    outputs = base_model.generate(inputs, max_new_tokens=4, attention_mask=attention_mask, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0168889a",
      "metadata": {
        "id": "0168889a"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"cpu\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
        "\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41f3091",
      "metadata": {
        "id": "b41f3091"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}